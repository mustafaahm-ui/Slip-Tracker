import streamlit as st
import cv2
import numpy as np
import time
from ultralytics import YOLO
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import av

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Live Tractor Speed Trap", layout="wide", page_icon="ğŸšœ")

# --- ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ---
if 'v_theo' not in st.session_state: st.session_state.v_theo = 0.0
if 'trap_distance' not in st.session_state: st.session_state.trap_distance = 20.0
if 'line1_percent' not in st.session_state: st.session_state.line1_percent = 20
if 'line2_percent' not in st.session_state: st.session_state.line2_percent = 80
if 'reset_trigger' not in st.session_state: st.session_state.reset_trigger = False

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªÙˆÙ‚Ù) ---
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ---
@st.cache_resource
def load_model():
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£Ø®Ù Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø³Ø±Ø¹Ø© (ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ±Ù‡ Ù„Ù€ best.pt Ù„Ø§Ø­Ù‚Ø§Ù‹)
        return YOLO('yolov8n.pt') 
    except:
        return None

model = load_model()

# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ---
class SpeedTrapProcessor(VideoTransformerBase):
    def __init__(self):
        self.model = model
        self.start_time = None
        self.end_time = None
        self.measured_speed = 0.0
        self.state = "WAITING"
        self.frame_count = 0 # Ø¹Ø¯Ø§Ø¯ Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø¶ØºØ·
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        self.l1_pct = st.session_state.get('line1_percent', 20)
        self.l2_pct = st.session_state.get('line2_percent', 80)
        self.dist = st.session_state.get('trap_distance', 20.0)

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        # 1. ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø­Ù„ Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¯)
        # Ù†Ù‚ÙˆÙ… Ø¨ØªØµØºÙŠØ±Ù‡Ø§ Ù„Ù„ØªØ­Ù„ÙŠÙ„ ÙÙ‚Ø·ØŒ Ù„ÙƒÙ† Ù†Ø¹Ø±Ø¶Ù‡Ø§ Ø¨Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£ØµÙ„ÙŠ Ø£Ùˆ Ø£ØµØºØ± Ù‚Ù„ÙŠÙ„Ø§Ù‹
        h_orig, w_orig, _ = img.shape
        img_resized = cv2.resize(img, (640, 480)) # Ø­Ø¬Ù… Ø®ÙÙŠÙ Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        
        # 2. ØªØ®Ø·ÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª (Frame Skipping)
        # Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙƒÙ„ 3 Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø­Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        self.frame_count += 1
        
        # Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ø±Ø³Ù…
        x1 = int(w_orig * (self.l1_pct / 100))
        x2 = int(w_orig * (self.l2_pct / 100))
        tractor_x = 0
        detected = False

        # --- Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ (ÙƒÙ„ 3 Ø¥Ø·Ø§Ø±Ø§Øª ÙÙ‚Ø·) ---
        if self.frame_count % 3 == 0:
            if self.model:
                results = self.model.track(img_resized, persist=True, verbose=False)
                if results[0].boxes.id is not None:
                    box = results[0].boxes.xyxy[0].cpu().numpy()
                    
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù…Ù† Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ØµØºÙŠØ± (640x480) Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£ØµÙ„ÙŠ
                    scale_x = w_orig / 640
                    scale_y = h_orig / 480
                    
                    x1_box = int(box[0] * scale_x)
                    y1_box = int(box[1] * scale_y)
                    x2_box = int(box[2] * scale_x)
                    y2_box = int(box[3] * scale_y)
                    
                    tractor_x = int((x1_box + x2_box) / 2)
                    detected = True
                    
                    # Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                    cv2.rectangle(img, (x1_box, y1_box), (x2_box, y2_box), (0, 255, 255), 2)
                    cv2.circle(img, (tractor_x, int((y1_box+y2_box)/2)), 8, (0, 0, 255), -1)

        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„ÙŠØ¯ÙˆÙŠ
        if st.session_state.get('reset_trigger', False):
            self.start_time = None
            self.end_time = None
            self.state = "WAITING"

        # --- Ù…Ù†Ø·Ù‚ Ø§Ù„ÙˆÙ‚Øª (ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ù…ÙˆÙ‚Ø¹ Ù…Ø¹Ø±ÙˆÙ) ---
        current_t = time.time()
        
        if self.state == "WAITING":
            if detected and tractor_x > x1: # ÙØ±Ø¶Ù†Ø§ Ø§Ù„Ø­Ø±ÙƒØ© Ù…Ù† Ø§Ù„ÙŠØ³Ø§Ø± Ù„Ù„ÙŠÙ…ÙŠÙ†
                self.start_time = current_t
                self.state = "RUNNING"
                
        elif self.state == "RUNNING":
            if detected and tractor_x > x2:
                self.end_time = current_t
                self.state = "FINISHED"
                duration = self.end_time - self.start_time
                if duration > 0.1: # Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
                    speed_ms = self.dist / duration
                    self.measured_speed = speed_ms * 3.6

        # --- Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø«Ø§Ø¨Øª (ÙŠØ¸Ù‡Ø± ÙÙŠ ÙƒÙ„ Ø¥Ø·Ø§Ø±) ---
        # Ø§Ù„Ø®Ø· 1
        cv2.line(img, (x1, 0), (x1, h_orig), (0, 255, 0), 2)
        cv2.putText(img, "START", (x1, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # Ø§Ù„Ø®Ø· 2
        cv2.line(img, (x2, 0), (x2, h_orig), (0, 0, 255), 2)
        cv2.putText(img, "FINISH", (x2, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        status_text = f"State: {self.state}"
        if self.state == "FINISHED":
            status_text += f" | Speed: {self.measured_speed:.2f} km/h"
            
        cv2.rectangle(img, (0, 0), (600, 60), (0, 0, 0), -1)
        cv2.putText(img, status_text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

        return img

# --- Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ---
st.title("ğŸšœ Live Speed Trap (Optimized)")

# Ø§Ù„ØªØ­ÙƒÙ…
c1, c2, c3 = st.columns([1, 2, 1])
st.session_state.trap_distance = c1.number_input("Distance (m)", 20.0)
st.session_state.line1_percent = c2.slider("Start Line", 5, 45, 20)
st.session_state.line2_percent = c2.slider("Finish Line", 55, 95, 80)

if c3.button("Reset System"):
    st.session_state.reset_trigger = True
    time.sleep(0.1)
    st.session_state.reset_trigger = False
    st.rerun()

# Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª
t1, t2 = st.tabs(["1. Asphalt (Theo)", "2. Field (Slip)"])

with t1:
    st.write("Measure Theoretical Speed:")
    # Ø¥Ø¶Ø§ÙØ© media_stream_constraints Ù„Ø·Ù„Ø¨ Ø¬ÙˆØ¯Ø© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø±Ø¹Ø©
    webrtc_streamer(
        key="cam1", 
        video_transformer_factory=SpeedTrapProcessor,
        rtc_configuration=RTC_CONFIGURATION,
        media_stream_constraints={"video": {"width": 640, "height": 480}, "audio": False}
    )
    
    manual_v = st.number_input("Recorded Speed (km/h):", 0.0)
    if st.button("Set Theoretical"):
        st.session_state.v_theo = manual_v
        st.success(f"Saved: {manual_v}")

with t2:
    if st.session_state.v_theo == 0:
        st.error("Go to Tab 1 first.")
    else:
        st.write(f"Reference Speed: **{st.session_state.v_theo} km/h**")
        webrtc_streamer(
            key="cam2", 
            video_transformer_factory=SpeedTrapProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": {"width": 640, "height": 480}, "audio": False}
        )
        
        act_v = st.number_input("Field Speed (km/h):", 0.0)
        if act_v > 0:
            slip = ((st.session_state.v_theo - act_v)/st.session_state.v_theo)*100
            st.metric("Slip %", f"{slip:.1f}%")